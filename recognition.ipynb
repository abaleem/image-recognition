{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-608238199241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_batch_1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mb'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mb'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-608238199241>\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bytes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_batch_1'"
     ]
    }
   ],
   "source": [
    "# Reading Dataset\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "dct = unpickle(\"data_batch_1\")\n",
    "X = dct[b'data']\n",
    "Y = np.array(dct[b'labels'])\n",
    "\n",
    "for i in range(2,5):\n",
    "    fn = \"data_batch_\" +str(i)\n",
    "    dct = unpickle(fn)\n",
    "    xTrain = np.vstack((X, dct[b'data']))\n",
    "    yTrain = np.concatenate((Y, dct[b'labels']))\n",
    "\n",
    "\n",
    "dct = unpickle(\"data_batch_5\")\n",
    "xValidate = dct[b'data']\n",
    "yValidate = np.array(dct[b'labels'])\n",
    "\n",
    "dct = unpickle(\"test_batch\")\n",
    "xTest = dct[b'data']\n",
    "yTest = np.array(dct[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining Models\n",
    "model1 = Sequential([\n",
    "    Dense(50, input_dim=3072),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])\n",
    "model2 = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(2,2), strides=1, activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(50),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])\n",
    "model3 = Sequential([\n",
    "    Conv2D(filters=64, kernel_size=(4,4), strides=1, activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(50),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nadam = optimizers.Nadam()\n",
    "model1.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "39840/40000 [============================>.] - ETA: 0s - loss: 1.8867 - acc: 0.3316Epoch 00001: val_loss improved from inf to 1.93743, saving model to weights.hdf5\n",
      "40000/40000 [==============================] - 14s 353us/step - loss: 1.8861 - acc: 0.3316 - val_loss: 1.9374 - val_acc: 0.3040\n",
      "Epoch 2/1000\n",
      "39904/40000 [============================>.] - ETA: 0s - loss: 1.8398 - acc: 0.3432Epoch 00002: val_loss improved from 1.93743 to 1.92460, saving model to weights.hdf5\n",
      "40000/40000 [==============================] - 12s 296us/step - loss: 1.8403 - acc: 0.3431 - val_loss: 1.9246 - val_acc: 0.3221\n",
      "Epoch 3/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 1.7993 - acc: 0.3588Epoch 00003: val_loss improved from 1.92460 to 1.83616, saving model to weights.hdf5\n",
      "40000/40000 [==============================] - 12s 293us/step - loss: 1.7993 - acc: 0.3589 - val_loss: 1.8362 - val_acc: 0.3534\n",
      "Epoch 4/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 1.7850 - acc: 0.3634Epoch 00004: val_loss improved from 1.83616 to 1.80309, saving model to weights.hdf5\n",
      "40000/40000 [==============================] - 12s 296us/step - loss: 1.7849 - acc: 0.3634 - val_loss: 1.8031 - val_acc: 0.3630\n",
      "Epoch 5/1000\n",
      "39904/40000 [============================>.] - ETA: 0s - loss: 1.7630 - acc: 0.3749Epoch 00005: val_loss improved from 1.80309 to 1.79407, saving model to weights.hdf5\n",
      "40000/40000 [==============================] - 12s 294us/step - loss: 1.7628 - acc: 0.3751 - val_loss: 1.7941 - val_acc: 0.3723\n",
      "Epoch 6/1000\n",
      "39840/40000 [============================>.] - ETA: 0s - loss: 1.7468 - acc: 0.3840Epoch 00006: val_loss improved from 1.79407 to 1.78048, saving model to weights.hdf5\n",
      "40000/40000 [==============================] - 12s 298us/step - loss: 1.7468 - acc: 0.3839 - val_loss: 1.7805 - val_acc: 0.3827\n",
      "Epoch 7/1000\n",
      "39904/40000 [============================>.] - ETA: 0s - loss: 1.7355 - acc: 0.3901Epoch 00007: val_loss did not improve\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 1.7351 - acc: 0.3905 - val_loss: 1.7995 - val_acc: 0.3628\n",
      "Epoch 8/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 1.7308 - acc: 0.3913Epoch 00008: val_loss did not improve\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 1.7307 - acc: 0.3913 - val_loss: 1.8786 - val_acc: 0.3405\n",
      "Epoch 9/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 1.7212 - acc: 0.3975Epoch 00009: val_loss did not improve\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 1.7214 - acc: 0.3974 - val_loss: 1.8089 - val_acc: 0.3606\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 1.4734 - acc: 0.4800Epoch 00001: val_loss improved from inf to 1.36516, saving model to weights1.hdf5\n",
      "40000/40000 [==============================] - 24s 610us/step - loss: 1.4731 - acc: 0.4800 - val_loss: 1.3652 - val_acc: 0.5238\n",
      "Epoch 2/1000\n",
      "39904/40000 [============================>.] - ETA: 0s - loss: 1.2252 - acc: 0.5720Epoch 00002: val_loss improved from 1.36516 to 1.28164, saving model to weights1.hdf5\n",
      "40000/40000 [==============================] - 23s 572us/step - loss: 1.2248 - acc: 0.5720 - val_loss: 1.2816 - val_acc: 0.5474\n",
      "Epoch 3/1000\n",
      "39904/40000 [============================>.] - ETA: 0s - loss: 1.1258 - acc: 0.6071Epoch 00003: val_loss improved from 1.28164 to 1.27659, saving model to weights1.hdf5\n",
      "40000/40000 [==============================] - 23s 578us/step - loss: 1.1261 - acc: 0.6070 - val_loss: 1.2766 - val_acc: 0.5605\n",
      "Epoch 4/1000\n",
      "39904/40000 [============================>.] - ETA: 0s - loss: 1.0468 - acc: 0.6334Epoch 00004: val_loss did not improve\n",
      "40000/40000 [==============================] - 23s 575us/step - loss: 1.0465 - acc: 0.6335 - val_loss: 1.2883 - val_acc: 0.5628\n",
      "Epoch 5/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 0.9859 - acc: 0.6567Epoch 00005: val_loss improved from 1.27659 to 1.14748, saving model to weights1.hdf5\n",
      "40000/40000 [==============================] - 23s 584us/step - loss: 0.9858 - acc: 0.6568 - val_loss: 1.1475 - val_acc: 0.6091\n",
      "Epoch 6/1000\n",
      "39904/40000 [============================>.] - ETA: 0s - loss: 0.9318 - acc: 0.6757Epoch 00006: val_loss improved from 1.14748 to 1.12823, saving model to weights1.hdf5\n",
      "40000/40000 [==============================] - 23s 587us/step - loss: 0.9318 - acc: 0.6757 - val_loss: 1.1282 - val_acc: 0.6079\n",
      "Epoch 7/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 0.8935 - acc: 0.6877Epoch 00007: val_loss did not improve\n",
      "40000/40000 [==============================] - 24s 603us/step - loss: 0.8936 - acc: 0.6878 - val_loss: 1.2591 - val_acc: 0.5798\n",
      "Epoch 8/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 0.8495 - acc: 0.7038Epoch 00008: val_loss did not improve\n",
      "40000/40000 [==============================] - 24s 607us/step - loss: 0.8494 - acc: 0.7039 - val_loss: 1.2034 - val_acc: 0.6029\n",
      "Epoch 9/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 0.8073 - acc: 0.7158Epoch 00009: val_loss did not improve\n",
      "40000/40000 [==============================] - 24s 591us/step - loss: 0.8075 - acc: 0.7158 - val_loss: 1.3031 - val_acc: 0.5830\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 1.4823 - acc: 0.4679Epoch 00001: val_loss improved from inf to 1.43681, saving model to weights2.hdf5\n",
      "40000/40000 [==============================] - 33s 826us/step - loss: 1.4820 - acc: 0.4680 - val_loss: 1.4368 - val_acc: 0.4868\n",
      "Epoch 2/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 1.2098 - acc: 0.5722Epoch 00002: val_loss improved from 1.43681 to 1.22472, saving model to weights2.hdf5\n",
      "40000/40000 [==============================] - 33s 826us/step - loss: 1.2093 - acc: 0.5724 - val_loss: 1.2247 - val_acc: 0.5732\n",
      "Epoch 3/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 1.1040 - acc: 0.6105Epoch 00003: val_loss improved from 1.22472 to 1.12910, saving model to weights2.hdf5\n",
      "40000/40000 [==============================] - 32s 812us/step - loss: 1.1044 - acc: 0.6103 - val_loss: 1.1291 - val_acc: 0.6068\n",
      "Epoch 4/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 1.0213 - acc: 0.6438Epoch 00004: val_loss did not improve\n",
      "40000/40000 [==============================] - 33s 820us/step - loss: 1.0213 - acc: 0.6438 - val_loss: 1.1884 - val_acc: 0.5867\n",
      "Epoch 5/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 0.9734 - acc: 0.6611Epoch 00005: val_loss did not improve\n",
      "40000/40000 [==============================] - 33s 824us/step - loss: 0.9736 - acc: 0.6611 - val_loss: 1.2005 - val_acc: 0.5944\n",
      "Epoch 6/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.9261 - acc: 0.6764Epoch 00006: val_loss improved from 1.12910 to 1.09586, saving model to weights2.hdf5\n",
      "40000/40000 [==============================] - 34s 842us/step - loss: 0.9260 - acc: 0.6764 - val_loss: 1.0959 - val_acc: 0.6183\n",
      "Epoch 7/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.8692 - acc: 0.6956Epoch 00007: val_loss did not improve\n",
      "40000/40000 [==============================] - 34s 844us/step - loss: 0.8695 - acc: 0.6955 - val_loss: 1.1191 - val_acc: 0.6130\n",
      "Epoch 8/1000\n",
      "39968/40000 [============================>.] - ETA: 0s - loss: 1.1154 - acc: 0.6108Epoch 00008: val_loss did not improve\n",
      "40000/40000 [==============================] - 34s 861us/step - loss: 1.1153 - acc: 0.6109 - val_loss: 1.4474 - val_acc: 0.4986\n",
      "Epoch 9/1000\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 1.1369 - acc: 0.6001Epoch 00009: val_loss did not improve\n",
      "40000/40000 [==============================] - 35s 870us/step - loss: 1.1365 - acc: 0.6003 - val_loss: 1.2877 - val_acc: 0.5801\n"
     ]
    }
   ],
   "source": [
    "# one-hot-encoding the classes.\n",
    "yTrainHot = to_categorical(yTrain, num_classes=10)\n",
    "yValidateHot = to_categorical(yValidate, num_classes=10)\n",
    "yTestHot = to_categorical(yTest, num_classes=10)\n",
    "\n",
    "# defining the stopping criteria.\n",
    "esc = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "# model 1\n",
    "cp = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "m1 = model1.fit(xTrain, yTrainHot, epochs=1000, callbacks=[esc, cp], validation_data=(xValidate, yValidateHot))\n",
    "\n",
    "# model 2\n",
    "xTrain = xTrain.reshape(xTrain.shape[0], 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "xValidate = xValidate.reshape(xValidate.shape[0], 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "cp = ModelCheckpoint(filepath='weights1.hdf5', verbose=1, save_best_only=True)\n",
    "m2 = model2.fit(xTrain, yTrainHot, epochs=1000, callbacks=[esc, cp], validation_data=(xValidate, yValidateHot))\n",
    "\n",
    "# model 3\n",
    "cp = ModelCheckpoint(filepath='weights2.hdf5', verbose=1, save_best_only=True)\n",
    "m3 = model3.fit(xTrain, yTrainHot, epochs=1000, callbacks=[esc, cp], validation_data=(xValidate, yValidateHot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 119us/step\n",
      "10000/10000 [==============================] - 2s 222us/step\n",
      "10000/10000 [==============================] - 3s 283us/step\n"
     ]
    }
   ],
   "source": [
    "# Testing with best weights learned during training.\n",
    "\n",
    "model1.load_weights('weights.hdf5')\n",
    "m1Score = model1.evaluate(xTest, yTestHot)\n",
    "\n",
    "xTest = xTest.reshape(xTest.shape[0], 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "model2.load_weights('weights1.hdf5')\n",
    "m2Score = model2.evaluate(xTest, yTestHot)\n",
    "\n",
    "model3.load_weights('weights2.hdf5')\n",
    "m3Score = model3.evaluate(xTest, yTestHot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7670069072723389, 0.37830000000000003]\n",
      "[1.1252307024002075, 0.61639999999999995]\n",
      "[1.097168815422058, 0.62019999999999997]\n"
     ]
    }
   ],
   "source": [
    "# Loss and accuracy for each model\n",
    "\n",
    "print(m1Score)\n",
    "print(m2Score)\n",
    "print(m3Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tech report (Chapter 3) describes the dataset and the methodology followed when collecting it in much greater detail. \n",
    "Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.\n",
    "https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
